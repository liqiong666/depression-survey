% !TEX root = tnnls_relation_gait.tex

\ifx\allfiles\undefined
    \input{tnnls_prefix}
\fi
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{table*}[htbp]
%	\caption{
%		The rank-1 accuracy (\%) on CASIA-B for different probe views excluding the identical-view cases.
%		For evaluation, the sequences of NM-1,2,3,4 for each subject are taken as the gallery.
%		% The probe contains the sequences of three walking conditions, \ie~NM, BG and CL.
%	}
%	\label{tab_acc_casia}
%	\begin{center}
%		\resizebox{0.9999\textwidth}{!}{%
%			\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c}
%				\hline
%				\multirow{2}{*}{\tabincell{c}{}} & \multirow{2}{*}{Method} & \multicolumn{11}{c|}{Probe View} & \multirow{2}{*}{Average} \\
%				\cline{3-13}
%				& & $0^{\circ}$ & $18^{\circ}$ & $36^{\circ}$ & $54^{\circ}$ & $72^{\circ}$ & $90^{\circ}$
%				& $108^{\circ}$ & $126^{\circ}$ & $134^{\circ}$ & $162^{\circ}$ &  $180^{\circ}$ & \\
%				\hline
%				\multirow{8}{*}{NM}
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				& GEINet~\cite{shiraga2016geinet}          & 40.20 & 38.90 & 42.90 & 45.60 & 51.20 & 42.00 & 53.50 & 57.60 & 57.80 & 51.80 & 47.70 & 48.11 \\
%				& CNN-LB~\cite{wu2016comprehensive}        & 82.60 & 90.30 & 96.10 & 94.30 & 90.10 & 87.40 & 89.90 & 94.00 & 94.70 & 91.30 & 78.50 & 89.93 \\
%				& GaitSet~\cite{chao2019gaitset}           & 93.40 & 98.10 & 98.50 & 97.80 & 92.60 & 90.90 & 94.20 & 97.30 & 98.40 & 97.00 & 89.10 & 95.21 \\
%				& GaitPart~\cite{fan2020gaitpart}          & 94.10 & 98.60 & 99.30 & 98.50 & 94.00 & 92.30 & 95.90 & 98.40 & 99.20 & 97.80 & 90.40 & 96.23 \\
%				& GLN~\cite{hou2020gait}                   & 93.20 & 99.30 & 99.50 & 98.70 & 96.10 & 95.60 & 97.20 & 98.10 & 99.30 & 98.60 & 90.10 & 96.88 \\
%				& SRN~\cite{hou2021setres}                 & 94.70 & 99.40 & 99.40 & 98.40 & 96.50 & 94.80 & 96.00 & 98.20 & 99.30 & 98.40 & 92.90 & 97.09 \\
%				& GQAN-Backbone(\bftab{ours})              & 95.80 & 99.60 & 99.70 & 99.10 & \bftab{97.90} & 96.30 & \bftab{97.80} & 98.70 & 99.50 & 98.30 & 94.10 & 97.89 \\
%				& GQAN(\bftab{ours})                       & \bftab{98.00} & \bftab{99.80} & \bftab{99.80} & \bftab{99.20} & 97.70 & \bftab{97.30} & \bftab{97.80} & \bftab{98.80} & \bftab{99.80} & \bftab{99.20} & \bftab{96.20} & \bftab{98.51} \\
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\hline
%				\multirow{8}{*}{BG}
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				& GEINet~\cite{shiraga2016geinet}          & 34.20 & 29.29 & 31.21 & 35.20 & 35.20 & 27.60 & 35.90 & 43.50 & 45.00 & 38.99 & 36.80 & 35.72 \\
%				& CNN-LB~\cite{wu2016comprehensive}        & 64.20 & 80.60 & 82.70 & 76.90 & 64.80 & 63.10 & 68.00 & 76.90 & 82.20 & 75.40 & 61.30 & 72.37 \\
%				& GaitSet~\cite{chao2019gaitset}           & 85.90 & 92.12 & 93.94 & 90.41 & 86.40 & 78.70 & 85.00 & 91.60 & 93.10 & 91.01 & 80.70 & 88.08 \\
%				& GaitPart~\cite{fan2020gaitpart}          & 89.10 & 94.80 & 96.70 & 95.10 & 88.30 & 84.90 & 89.00 & 93.50 & 96.10 & 93.80 & 85.80 & 91.55 \\
%				& GLN~\cite{hou2020gait}                   & 91.10 & 97.68 & 97.78 & 95.20 & 92.50 & 91.20 & 92.40 & \bftab{96.00} & 97.50 & 94.95 & 88.10 & 94.04 \\
%				& SRN~\cite{hou2021setres}                 & 92.00 & 97.37 & 97.58 & 95.82 & 91.80 & 90.40 & 93.20 & 95.30 & 97.60 & 95.35 & 87.80 & 94.02 \\
%				& GQAN-Backbone(\bftab{ours})              & 93.90 & 97.27 & 97.37 & 96.43 & \bftab{94.00} & \bftab{92.60} & 93.10 & 95.40 & 97.40 & 96.97 & 88.70 & 94.83 \\
%				& GQAN(\bftab{ours})                       & \bftab{96.00} & \bftab{98.69} & \bftab{98.38} & \bftab{96.94} & 93.40 & 90.80 & \bftab{93.70} & 95.90 & \bftab{97.70} & \bftab{97.07} & \bftab{90.50} & \bftab{95.37} \\
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\hline
%				\multirow{8}{*}{CL}
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				& GEINet~\cite{shiraga2016geinet}          & 19.90 & 20.30 & 22.50 & 23.50 & 26.70 & 21.30 & 27.40 & 28.20 & 24.20 & 22.50 & 21.60 & 23.46 \\
%				& CNN-LB~\cite{wu2016comprehensive}        & 37.70 & 57.20 & 66.60 & 61.10 & 55.20 & 54.60 & 55.20 & 59.10 & 58.90 & 48.80 & 39.40 & 53.98 \\
%				& GaitSet~\cite{chao2019gaitset}           & 63.70 & 75.60 & 80.70 & 77.50 & 69.10 & 67.80 & 69.70 & 74.60 & 76.10 & 71.10 & 55.70 & 71.05 \\
%				& GaitPart~\cite{fan2020gaitpart}          & 70.70 & 85.50 & 86.90 & 83.30 & 77.10 & 72.50 & 76.90 & 82.20 & 83.80 & 80.20 & 66.50 & 78.69 \\
%				& GLN~\cite{hou2020gait}                   & 70.60 & 82.40 & 85.20 & 82.70 & 79.20 & 76.40 & 76.20 & 78.90 & 77.90 & 78.70 & 64.30 & 77.50 \\
%				& SRN~\cite{hou2021setres}                 & 75.10 & 88.20 & 89.90 & 86.30 & 81.20 & 78.80 & 80.00 & 84.00 & 86.30 & 80.70 & 68.80 & 81.75 \\
%				& GQAN-Backbone(\bftab{ours})              & 71.70 & 84.00 & 88.70 & 84.30 & 83.20 & 78.30 & 81.80 & 83.20 & 83.60 & 77.50 & 66.10 & 80.22 \\
%				& GQAN(\bftab{ours})                       & \bftab{80.20} & \bftab{90.30} & \bftab{90.20} & \bftab{87.40} & \bftab{85.50} & \bftab{81.50} & \bftab{83.70} & \bftab{85.30} & \bftab{86.90} & \bftab{83.30} & \bftab{75.30} & \bftab{84.51} \\
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\hline
%			\end{tabular}
%		}
%	\end{center}
%\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}

\subsection{Experimental Settings}
The experimental settings for GQAN are similar to the baseline methods~\cite{hou2020gait,hou2021setres,chao2019gaitset} to ensure the fair comparisons.
%
The methods in~\cite{hou2020gait,hou2021setres} are our previous works which aim to learn more discriminative features from the silhouettes for gait recognition.
%
Differently, the main goal of GQAN is to enhance the \emph{interpretability} of silhouette-based gait recognition, which also achieves very competitive performance under all walking conditions.

\subsubsection{GQAN-Backbone}
\label{sec_settings_backbone}
In our experiments, we design an effective and efficient backbone for GQAN.
%
Specifically, GQAN-Backbone is modified from GaitSet~\cite{chao2019gaitset} and our modifications mainly lie in the follows:
\begin{enumerate}[(i)]
	\item We use $S \! = \! 16$ instead of $S \!= \! \{1, 2, 4, 8, 16\}$ for simplicity to horizontally slice the features in Horizontal Pyramid Matching.
	\item We remove Multilayer Global Pipeline to make it feasible to separately assess the quality of each silhouette in the high layers, which can also accelerate the training and reduce the GPU memory consumption.
	\item We add the BNNeck~\cite{luo2019bag} and compute the cross-entropy loss on the concatenated features of all parts.
	\item We use the warmup strategy~\cite{he2016deep} to adjust the learning rate at the start of training.
	\item We adopt the random erasing data augmentation~\cite{zhong2020random} to alleviate the overfitting on CASIA-B~\cite{yu2006framework}.
	\item We add two additional convolutional layers in the Encoder for the experiments on OUMVLP~\cite{takemura2018multi} to adapt to the large-scale dataset.
\end{enumerate}
It is worth noting that, the networks in GaitSet~\cite{chao2019gaitset} as well as~\cite{fan2020gaitpart,hou2020gait,hou2021setres} cannot be directly adopted as the backbone for GQAN.
%
Specifically, the networks in GaitSet~\cite{chao2019gaitset}, GLN~\cite{hou2020gait} and SRN~\cite{hou2021setres} consist of a global branch (\eg, Multilayer Global Pipeline in GaitSet~\cite{chao2019gaitset}) to aggregate the silhouette-level features at the early layers which makes it infeasible to separately assess the quality of each silhouette in the high layers.
%
Besides, GaitPart~\cite{fan2020gaitpart} relies on a MCM module to model the micro-motion features in the adjacent frames and we provide the comparison between FQBlock and MCM in Section~\ref{sec_mcm_comparison}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Datasets}
The experiments are mainly conducted on two typical gait datasets, \ie, CASIA-B~\cite{yu2006framework} and OUMVLP~\cite{takemura2018multi}.
%
% The dataset statistics are summarized in Table~\ref{tab_dataset}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
CASIA-B consists of $124$ subjects and collects the videos of normal walking (NM-1,2,3,4,5,6), walking with bags (BG-1,2) and walking in different coats/jackets (CL-1,2).
%
There are $11$ views for each walking condition.
%
Since there is not split way provided in the dataset, we take the first $74$ subjects as training set with the rest $50$ subjects as test set.
%
For evaluation, the sequences of NM-1,2,3,4 for each subject are taken as the gallery and the sequences of NM-5,6, BG-1,2, CL-1,2 are taken as the probe.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
OUMVLP consists of $10307$ subjects which is the largest public gait dataset so far.
%
However, it only provides the silhouettes of normal walking (NM-00,01) for each subject.
%
There are $14$ views available for normal walking.
%
According to the split way provided in the dataset, we take the $5153$ subjects as training set with the rest $5154$ subjects as test set.
%
For evaluation, the sequences of NM-01 for each subject are taken as the gallery and the sequences of NM-00 are taken as the probe.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Implementation Details}
All models are implemented with PyTorch~\cite{NeurIPS2019_9015} and trained on TITAN-V GPUs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The silhouettes in both datasets are pre-processed using the method in~\cite{takemura2017input}.
%
The input size of each silhouette is set to $128 \times 88$ for CASIA-B and $64 \times 44$ for OUMVLP.
%
In the training phase, we randomly select $30$ silhouettes from each sequence as the input.
%
The number of the subjects and the number of the sequences for each subject in a batch are set to $(8, 16)$ for CASIA-B and $(32, 16)$ for OUMVLP.
%
For evaluation, all silhouettes for each sequence are taken as the input to obtain the representations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The convolutional channels are set to $\{32, 64, 128\}$ for CASIA-B and $\{64, 128, 256, 512\}$ for OUMVLP.
%
The output dimension for each part representation is set to $256$.
%
For FQBlock, there is no dimension reduction and the channels in the two fully connected layers are the same as the output channel of the last convolutional layer ($128$ for CASIA-B and $512$ for OUMVLP).
%
For PQBlock, the output dimension of the fully connected layer is set to $1$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
SGD with momentum is taken as the optimizer.
%
The learning rate is initialized with $0.1$ and scaled to its $1/10$ three times for the training.
%
The stepsize is set to $10000$ iterations for CASIA-B and $50000$ iterations for OUMVLP.
%
The momentum and the weight decay are set to $0.9$ and $0.0005$ for the optimization.
%
Particularly, GQAN is pretrained without PQLoss using the initial learning rate $0.1$ for $10000$ iterations for CASIA-B and $50000$ iterations for OUMVLP.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Besides, the margin thresholds $m_{ap}$ and $m_{an}$ in Eq~\eqref{eq_pquality_lpq} are both set to $0.01$,
%
the margin threshold $m$ in Eq~\eqref{eq_summary_tp} is set to $0.2$,
%
the loss weight $\alpha$ in Eq~\eqref{eq_summary_l1} and Eq~\eqref{eq_summary_l2} is set $0.1$,
%
the loss weight $\beta$ in Eq~\eqref{eq_summary_l2} is set to $10.0$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[!tbp]
	\caption{
		The rank-1 accuracy (\%) on OUMVLP for different probe views excluding the identical-view cases.
		For evaluation, the sequences of NM-01 for each subject are taken as the gallery.
        The probe sequences which have no corresponding ones in the gallery are included and ignored respectively.
		% The last three rows show the results ignoring the probe sequences which have no corresponding ones in the gallery.
	}
	\label{tab_acc_oumvlp}
	\begin{center}
		\resizebox{0.9999\textwidth}{!}{%
			\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
				\hline
				\multirow{2}{*}{Method} & \multicolumn{14}{c|}{Probe View} & \multirow{2}{*}{Average} \\
				\cline{2-15}
				& $0^{\circ}$ & $15^{\circ}$ & $30^{\circ}$ & $45^{\circ}$ & $60^{\circ}$ & $75^{\circ}$ & $90^{\circ}$
				& $180^{\circ}$ & $195^{\circ}$ & $210^{\circ}$ & $225^{\circ}$ &  $240^{\circ}$ &  $255^{\circ}$ &  $270^{\circ}$ & \\
				\hline
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				GEINet~\cite{shiraga2016geinet}          & 23.20 & 38.09 & 47.95 & 51.81 & 47.53 & 48.09 & 43.75 & 27.25 & 37.89 & 46.78 & 49.85 & 45.94 & 45.65 & 40.96 & 42.48 \\
				GaitSet~\cite{chao2019gaitset}           & 79.33 & 87.59 & 89.96 & 90.09 & 87.96 & 88.74 & 87.69 & 81.82 & 86.46 & 88.95 & 89.17 & 87.16 & 87.60 & 86.15 & 87.05 \\
				GaitPart~\cite{fan2020gaitpart}          & 82.57 & 88.93 & 90.84 & 91.00 & 89.75 & 89.91 & 89.50 & 85.19 & 88.09 & 90.02 & 90.15 & 89.03 & 89.10 & 88.24 & 88.74 \\
				GLN~\cite{hou2020gait}                   & 83.81 & 90.00 & 91.02 & 91.21 & 90.25 & 89.99 & 89.43 & 85.28 & 89.09 & \bftab{90.47} & 90.59 & 89.60 & 89.31 & 88.47 & 89.18 \\
				SRN~\cite{hou2021setres}                 & 83.76 & 89.70 & 90.94 & 91.19 & 89.88 & 90.25 & 89.61 & 85.76 & 88.79 & 90.11 & 90.41 & 89.03 & 89.36 & 88.47 & 89.09 \\
				GQAN-Backbone(\bftab{ours})              & 84.38 & 90.06 & 91.15 & 91.30 & 90.41 & 90.41 & 89.86 & 86.85 & 89.13 & 90.34 & 90.51 & 89.75 & 89.50 & 88.77 & 89.46 \\
				GQAN(\bftab{ours})                       & \bftab{84.99} & \bftab{90.34} & \bftab{91.26} & \bftab{91.40} & \bftab{90.63} & \bftab{90.57} & \bftab{90.14} & \bftab{87.09} & \bftab{89.37} & 90.46 & \bftab{90.64} & \bftab{90.02} & \bftab{89.81} & \bftab{89.10} & \bftab{89.70} \\
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\hline
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				GEINet~\cite{shiraga2016geinet}          & 24.91 & 40.65 & 51.55 & 55.13 & 49.81 & 51.05 & 46.37 & 29.17 & 40.67 & 50.53 & 53.27 & 48.39 & 48.64 & 43.49 & 45.26 \\
				GaitSet~\cite{chao2019gaitset}           & 84.50 & 93.27 & 96.72 & 96.58 & 93.48 & 95.28 & 94.15 & 87.04 & 92.50 & 96.00 & 95.96 & 92.99 & 94.34 & 92.69 & 93.25 \\
				GaitPart~\cite{fan2020gaitpart}          & 87.95 & 94.70 & 97.69 & 97.59 & 95.46 & 96.60 & 96.15 & 90.61 & 94.25 & 97.17 & 97.06 & 95.07 & 96.02 & 95.02 & 95.10 \\
				GLN~\cite{hou2020gait}                   & 89.28 & 95.84 & 97.87 & 97.82 & 96.01 & 96.68 & 96.07 & 90.71 & 95.34 & \bftab{97.66} & 97.54 & 95.69 & 96.24 & 95.27 & 95.57 \\
				SRN~\cite{hou2021setres}                 & 89.22 & 95.52 & 97.79 & 97.81 & 95.62 & 96.97 & 96.28 & 91.22 & 95.01 & 97.26 & 97.35 & 95.07 & 96.31 & 95.28 & 95.48 \\
				GQAN-Backbone(\bftab{ours})              & 89.88 & 95.92 & 98.03 & 97.94 & 96.20 & 97.17 & 96.57 & 92.37 & 95.37 & 97.51 & 97.46 & 95.88 & 96.48 & 95.62 & 95.89 \\
				GQAN(\bftab{ours})                       & \bftab{90.53} & \bftab{96.20} & \bftab{98.14} & \bftab{98.04} & \bftab{96.44} & \bftab{97.34} & \bftab{96.88} & \bftab{92.63} & \bftab{95.63} & 97.64 & \bftab{97.61} & \bftab{96.18} & \bftab{96.82} & \bftab{95.98} & \bftab{96.15} \\
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\hline
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\end{tabular}
		}
	\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance Comparison}

\subsubsection{CASIA-B}
Table~\ref{tab_acc_casia} shows the performance comparison on CASIA-B.
%
The probe sequences are divided into three categories according to the walking conditions, \ie, NM, BG, CL, which are respectively evaluated.
%
We report the rank-1 accuracy for each probe view averaged on all gallery views excluding the identical-view cases~\cite{wu2016comprehensive}.

In the methods listed in Table~\ref{tab_acc_casia}, GEINet~\cite{shiraga2016geinet} and CNN-LB~\cite{wu2016comprehensive} are two representative methods taking GEIs as the input.
%
GaitSet~\cite{chao2019gaitset} first proposes to treat the silhouettes of a gait sequence as an unordered set and horizontally slices the features to learn part representation for gait recognition, which achieves significant improvement compared to the GEI-based methods.
%
GaitPart~\cite{fan2020gaitpart} uses a Focal Convolutional Layer and Micro-Motion Capture Module to enhance the part representations.
%
GLN~\cite{hou2020gait} takes the lateral connections to merge multi-layer features and proposes a Compact Block to reduce the representation dimension.
%
% For a comprehensive study, we also provide the comparison with the backbone of GLN without Compact Block (denoted as GLN-Backbone).
SRN~\cite{hou2021setres} proposes a Set Residual Block to effectively coordinate the silhouette-level and set-level information in the feature learning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For the results in Table~\ref{tab_acc_casia}, we can observe that the backbone we design for GQAN achieves the competitive performance (NM-$97.89\%$, BG-$94.83\%$, CL-$80.22\%$) compared to the previous works.
%
FQBlock and PQBlock for GQAN, which are proposed to assess the quality of each silhouette and each part for gait recognition, can further boost the performance under all walking conditions to state-of-the-art (NM-$98.51\%$, BG-$95.37\%$, CL-$84.51\%$).
%
Particularly, under the most challenging condition of walking in different coats/jackets, the rank-1 accuracy achieved by GQAN exceeds GQAN-Backbone by a large margin ($+4.29\%$).
%
In Section~\ref{sec_settings_backbone}, we have explained why the networks in~\cite{chao2019gaitset,fan2020gaitpart,hou2020gait,hou2021setres} cannot be directly adopted as the backbone for GQAN.
%
While in Table~\ref{tab_acc_casia}, GQAN-Backbone and GQAN constitute a fair comparison, and the performance gain brought by GQAN validates the effectiveness of FQBlock and PQBlock.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!tbp]
	\caption{
	The performance comparison on HID Competition Dataset 2021. The results are reported in the rank-1 accuracy.
	}
	\label{tab_hid}
	\begin{center}
		\resizebox{0.9999\linewidth}{!}{%
			\begin{tabular}{c|c|c|c}
				\hline
				Method & ~~~~SRN~\cite{hou2021setres}~~~~ & GQAN-Backbone & ~~~~~GQAN~~~~~ \\
				\hline
				Rank-1 Acc & 64.31 & 58.19 & \bftab{65.61} \\
				\hline
			\end{tabular}
		}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{OUMVLP}
Table~\ref{tab_acc_oumvlp} shows the performance comparison on OUMVLP.
%
Though a large number of subjects are available, the lack of walking with bags (BG) and walking in different clothes (CL) makes it less challenging than CASIA-B.
%
Due to the incomplete data for some subjects, we respectively conduct the evaluation \emph{including} and \emph{ignoring} the probe sequences which have no corresponding ones in the gallery.

Compared to the methods listed in Table~\ref{tab_acc_casia}, CNN-LB~\cite{wu2016comprehensive} is too time-consuming for training and test which is thus not listed in Table~\ref{tab_acc_oumvlp} for the large-scale dataset.
%
% As far as we know, GLN~\cite{hou2020gait} holds the best performance (\ie, $95.57\%$) on OUMVLP before this work.
%
% In comparison, GQAN-Backbone achieves the rank-1 accuracy of $95.89\%$ and outperforms all the baselines, which validates the effectiveness of the backbone we designed for GQAN.
%
Here we mainly compare the rank-1 accuracy obtained by \emph{ignoring} the probe sequences which do not have the corresponding ones in the gallery.
%
From the results in Table~\ref{tab_acc_oumvlp}, we can observe that the baselines including GaitPart~\cite{fan2020gaitpart}, GLN~\cite{hou2020gait}, SRN~\cite{hou2021setres} all report the rank-1 accuracy of more than $95\%$ on this dataset.
%
Particularly, GQAN-Backbone achieves the competitive accuracy of $95.89\%$ which validates the effectiveness of the backbone we designed for GQAN.
%
Besides, we notice that the frame quality of each silhouette in OUMVLP is obviously higher than that in CASIA-B and the silhouettes for each subject walking in different clothes (CL) are not available.
%
FQBlock and PQBlock, which work by explicitly assessing the quality of each silhouette and each part, can still benefit the gait recognition and improve the rank-1 accuracy to $96.15\%$.

\subsubsection{HID Competition Dataset 2021}
CASIA-B and OUMVLP are the two most popular benchmarks which are widely used in previous literature~\cite{chao2019gaitset,fan2020gaitpart,hou2020gait,hou2021setres}.
%
The gait dataset is lacking due to the privacy issue, the requirement for cameras and sites.
%
To further verify the effectiveness of GQAN, we conduct the experiments on HID Competition Dataset 2021~\cite{yu2021hid} using the settings similar to CASIA-B.
%
The competition provides the sequences for 500 subjects and each subject consists of about 10 sequences.
%
We take the sequences of the first 300 subjects as training set and the rest 200 subjects as test set.
%
For evaluation, we gather the first sequence of each subject as the gallery and take the rest sequences as the probe.
%
The performance comparison is provided in Table~\ref{tab_hid} and the results are reported in the rank-1 accuracy.
%
Along with GQAN-Backbone and GQAN, we also re-implement SRN on this dataset which holds the best performance for gait recognition before this work.
%
The performance comparison shown in Table~\ref{tab_hid}, especially the performance gain from GQAN-Backbone to GQAN, further validates the effectiveness of the proposed method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!tbp]
	\caption{
		The effect of each block for GQAN. The results are reported in the rank-1 accuracy excluding the identical-view cases.
		% \emph{GQAN-B} for \emph{GQAN-Backbone}.
	}
	\label{tab_ablation}
	\begin{center}
		\resizebox{0.9999\linewidth}{!}{%
			\begin{tabular}{c|ccc|c}
				\hline
				Dataset & \multicolumn{3}{c|}{CASIA-B} & OUMVLP \\
				\hline
				Method  			 		 & NM & BG & CL & NM \\
				\hline
				GQAN-Backbone        		 & 97.89 & 94.83 & 80.22 & 95.89 \\
				GQAN-Backbone+FQBlock 		 & 98.60 & 95.41 & 83.72 & 96.01 \\
				GQAN-Backbone+PQBlock		 & 97.96 & 94.68 & 82.36 & 96.01 \\
				GQAN           		         & 98.51 & 95.37 & 84.51 & 96.15 \\
				\hline
			\end{tabular}
		}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[tbp]
	\centering
	\subfloat[ID=110, Type=BG-01, View=036, Bin=1]{
		\label{fig_fquality1}\includegraphics[width=0.492\linewidth]{figures/png_id_110_type_bg-01_view_036_visualize_fquality_bin00.pdf}
	}
	\subfloat[ID=110, Type=BG-01, View=090, Bin=15]{
		\label{fig_fquality2}\includegraphics[width=0.492\linewidth]{figures/png_id_110_type_bg-01_view_090_visualize_fquality_bin14.pdf}
	}
	\\
	\vspace{-2mm}
	\subfloat[ID=112, Type=CL-02, View=018, Bin=1]{
		\label{fig_fquality3}\includegraphics[width=0.492\linewidth]{figures/png_id_112_type_cl-02_view_018_visualize_fquality_bin00.pdf}
	}
	\subfloat[ID=112, Type=NM-06, View=144, Bin=11]{
		\label{fig_fquality4}\includegraphics[width=0.492\linewidth]{figures/png_id_112_type_nm-06_view_144_visualize_fquality_bin10.pdf}
	}
	\\
	\vspace{-2mm}
	\subfloat[ID=123, Type=BG-02, View=018, Bin=4]{
		\label{fig_fquality5}\includegraphics[width=0.492\linewidth]{figures/png_id_123_type_bg-02_view_018_visualize_fquality_bin03.pdf}
	}
	\subfloat[ID=123, Type=CL-02, View=018, Bin=4]{
		\label{fig_fquality6}\includegraphics[width=0.492\linewidth]{figures/png_id_123_type_cl-02_view_018_visualize_fquality_bin03.pdf}
	}
	\caption{
		Examples for frame quality visualization on CASIA-B.
		%
		The silhouettes in each figure are randomly selected from each gait sequence and sorted in a descending order according to the frame quality scores of a certain bin.
		%
		The corresponding region in each silhouette is marked in red.
		%
		The number above each silhouette is the frame quality score which is computed by adding $Y_{ij}$ in Eq~\eqref{eq_fquality_yij} along the channel dimension.
		%
		% Best viewed in color.
	}
	\label{fig_fquality}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Study}
In this section, we conduct more experiments to further analyze GQAN.
% and the experiments are mainly conducted on CASIA-B covering different walking conditions.
%
For simplicity, we report the rank-1 accuracy under different walking conditions averaged on all probe and gallery views excluding the identical-view cases.

\subsubsection{The Effect of Each Block}
GQAN mainly consists of two blocks, \ie, FQBlock and PQBlock, to explicitly assess the quality of each silhouette and each part.
%
It is proposed towards the interpretability of silhouette-based gait recognition which also achieves very competitive performance.
%
Here we conduct the experiments to separately evaluate the effect of each block on the recognition accuracy.
%
The experimental results are provided in Table~\ref{tab_ablation}.
%
From the results on CASIA-B, we can observe that FQBlock can improve the performance for all walking conditions (NN, BG, CL).
%
The probable reason is that the silhouettes in CASIA-B are obtained by subtracting the background~\cite{yu2006framework} and contain a lot of noise.
%
PQBlock is mainly beneficial for walking in different coats/jackets (CL) which causes a lot of shape variance for the upper body.
%
For OUMVLP, as mentioned above, the overall improvement is not that significant due to the high quality of each silhouette and the lack of walking in different clothes (CL), while the performance comparison shown in Table~\ref{tab_ablation} indicates that FQBlock and PQBlock can still help improve the recognition accuracy respectively.
%
% the quality of each silhouette is relatively high and only the silhouettes of normal walking (NM) are available for each subject, while FQBlock and PQBlock can still help improve the recognition accuracy.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tbp]
	\caption{
		The performance comparison under the low-resolution conditions.
		The experiments are conducted on the test set of CASIA-B and the results are reported in the rank-1 accuracy excluding the identical-view cases.
	}
	\label{tab_diff_resolution}
	\begin{center}
		\resizebox{0.9999\linewidth}{!}{%
			\begin{tabular}{c|c|ccc}
				\hline
				Downsample Size & ~~~~~~~Method~~~~~~~ & NM & BG & CL \\
				\hline
				\multirow{2}{*}{-}
				& GQAN-Backbone & 97.89 & 94.83 & 80.22 \\
				& GQAN          & 98.51 & 95.37 & 84.51 \\
				\hline
				\multirow{2}{*}{ $64\times44$ }
				& GQAN-Backbone & 96.77 & 93.29 & 74.93 \\
				& GQAN          & 97.89 & 94.16 & 79.67 \\
				\hline
				\multirow{2}{*}{ $32\times22$ }
				& GQAN-Backbone & 72.11 & 65.11 & 37.29 \\
				& GQAN          & 76.63 & 69.85 & 44.10 \\
				\hline
			\end{tabular}
		}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Performance Comparison under Low Resolution}
In this section, we provide the performance comparison between GQAN-Backbone and GQAN under the low-resolution conditions for a more comprehensive study.
%
Specifically, the experiments are conducted on the test set of CASIA-B covering different walking conditions.
%
In the above experiments, the input size of each silhouette is set to $128 \times 88$ for training and test.
%
While in this section, we first downsample the silhouettes in the test set and then restore to $128 \times 88$ to simulate the low-resolution conditions.
%
The results shown in Table~\ref{tab_diff_resolution} show that GQAN can consistently outperform the backbone, which indicates that the proposed method is more robust to the low-resolution conditions.

\subsubsection{Frame Quality Visualization}
\label{sec_frame_vis}
As described in Section~\ref{sec_fquality}, FQBlock works in a squeeze-and-excitation style to assess the frame quality of each silhouette for gait recognition.
%
Specifically, it predicts the scores $Y_{ij}$ as shown in Eq~\eqref{eq_fquality_yij} to recalibrate the features of the $i$-th silhouette and $j$-bin,
%
and we add $Y_{ij}$ along the channel dimension as the frame quality indicator for the $i$-th silhouette and $j$-th region.
%
In Fig.~\ref{fig_fquality}, we present some examples from CASIA-B and the silhouettes are sorted in descending order according to the predicted scores of FQBlock.
%
As aforementioned, the silhouettes in CASIA-B contain a lot of noise due to the errors in the background extraction~\cite{yu2006framework}.
%
From the results in Fig.~\ref{fig_fquality}, we can observe that $Y_{ij}$ can be treated as an indicator of the frame quality for each silhouette in a gait sequence.
%
% As mentioned in Section~\ref{sec_fquality}, FQBlock holds independent weights for different bins obtained by horizontally and equally slicing the features, and thus the orders of the silhouettes are different according to the $Y_{ij}$ with different subscript $j$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/casia_b_visualize_pquality_cross.pdf}
	\caption{
		The statistics of part quality scores on CASIA-B.
		We conduct the experiment on the test set and average the scores for each part when computing the distance of gait sequences belonging to different types.
		\emph{REFER} for \emph{the reference scores of treating all parts equally (0.0625).}
	}
	\label{fig_pquality_cross}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Part Quality Visualization}
As described in Section~\ref{sec_pquality}, PQBlock operates on the set-level part representations and it predicts a score encoding the part quality to generate the adaptive weights as shown in Eq~\eqref{eq_pquality_ada}.
%
For the visualization of PQBlock, we obtain the statistics of part quality scores on the test set of CASIA-B when computing the distance of gait sequences belonging to different walking conditions, including NM-NM, NM-BG, NM-CL.
%
The results are displayed in Fig.~\ref{fig_pquality_cross}.
%
It can be observed that the part quality scores for the upper body ($6$-th to $10$-th part) are relatively low especially for the cases of NM-CL.
%
% The main reason is that the subjects of CASIA-B only change coats or jackets while the pants or skirts are the same for different walking conditions.
%
Besides, the mean score of $15$-th part is relatively smaller than the reference score of treating all parts equally, which is possibly caused by the segmentation errors due to the shadow in the floor~\cite{yu2006framework}.
%
In Fig.~\ref{fig_shadow_errors}, we display some example silhouettes from CASIA-B to illustrate the segmentation errors caused by the shadow in the floor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/casia_b_visualize_shadow_errors_bin14.pdf}
	\caption{
		Example silhouettes from CASIA-B.
		The silhouettes in CASIA-B contain a lot of noise for the $15$-th region due to the shadow in the floor.
		The corresponding region in each silhouette is marked in red.
		% Best viewed in color.
	}
	\label{fig_shadow_errors}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}
\label{sec_discussion}
\subsubsection{Comparison with MCM}
\label{sec_mcm_comparison}
In this section, we provide the comparison between FQBlock and MCM~\cite{fan2020gaitpart} which both take the part features extracted by the backbone as the input.
%
Firstly, the motivation behind the two blocks is different. MCM is designed to model the micro-motion features in the adjacent frames while FQBlock is proposed to measure the quality of each frame.
%
Secondly, the working mechanism of the two blocks is different. For simplicity, given a gait sequence, we denote the three dimensions of tensor to FQBlock and MCM as \emph{D-S}, \emph{D-P}, \emph{D-C}. Specifically, \emph{D-S} denotes the number of silhouettes, \emph{D-P} denotes the number of human parts, \emph{D-C} denotes the number of feature channels. MCM mainly works on \emph{D-S} dimension to deal with the relation of the adjacent frames, while FQBlock works on \emph{D-C} dimension to deal with the features of each frame separately.
%
Thirdly, the attention values in the two blocks have different characteristics. Specifically, the attention values of each frame in MCM rely on the adjacent ones, which are susceptible to temporal kernel size, silhouette order and missing frames. Moreover, the attention values for the frames in different sequences are not comparable due to the variation of context. As a result, the attention values in MCM are not suitable as the frame quality indicator. In contrast, the attention values of each frame in FQBlock only rely on the features of itself and are permutation invariant to silhouette order. Besides, FQBlock shares the weights across different silhouettes, which makes the attention values of the frames in different sequences comparable. The visualization results shown in Fig.~\ref{fig_fquality} indicate that the attention values in FQBlock can be taken as the frame quality indicator.

In addition, we conduct the experiment with MCM using the proposed strong baseline on CASIA-B. The performance of GQAN-Backbone+MCM (NM-97.96\%, BG-94.92\%, CL-81.74\%) is inferior to GQAN-Backbone+FQBlock (NM-98.60\%, BG-95.41\%, CL-83.72\%), which further demonstrates the effectiveness of FQBlock.

\subsubsection{Comparison with More Methods}
For a comprehensive study, we provide the performance comparison with more methods on CASIA-B and OUMVLP in Table~\ref{tab_more_baselines}.
%
Most of these works are orthogonal to GQAN such as the model-based methods (\eg, PoseGait~\cite{liao2020model} and End2EndGait~\cite{li2020end}) and the appearance methods taking other types of input for gait recognition (\eg, GaitNet~\cite{zhang2019gait} and GaitMotion~\cite{bashir2009gait}).
%
Particularly, SM-Prod~\cite{castro2020multimodal} reporting a little higher accuracy for NM and BG on CASIA-B. However, the optical flow needs a lot of computation cost and the performance for the most challenging CL is much inferior to GQAN.
%
Besides, SRN+CBlock~\cite{hou2021setres} is a variant of SRN which integrates SRN with Compact Block proposed in~\cite{hou2020gait}.
%
It reports a little higher accuracy on OUMVLP while its performance on CASIA-B is inferior to GQAN especially for the challenging CL ($77.7\%$~\vs~$84.51\%$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[tbp]
	\caption{
		The performance comparison with more baselines.
		The results are reported in the rank-1 accuracy excluding the identical-view cases.
		Sil for Silhouettes, RGB for RGB Frames, OF for Optical Flow.
	}
	\label{tab_more_baselines}
	\begin{center}
		\resizebox{0.9999\linewidth}{!}{%
			\begin{tabular}{c|c|c|ccc}
				\hline
				Dataset & Method & Input & NM & BG & CL \\
				\hline
				\multirow{11}{*}{CASIA-B}
				& J-CNN~\cite{zhang2019comprehensive}     & Sil & 91.2 & 75.0 & 54.0 \\
				& GaitSet-L~\cite{hou2020gait}   		  & Sil & 95.6 & 91.5 & 75.3 \\
				& GLN-Backbone~\cite{hou2020gait} 		  & Sil & 95.5 & 92.0 & 77.2 \\
				& SRN+CBlock~\cite{hou2021setres}         & Sil & 97.5 & 94.3 & 77.7 \\
				& MT3D~\cite{lin2020gait}	              & Sil & 96.7 & 93.0 & 81.5 \\
				& PoseGait~\cite{liao2020model}           & RGB & 68.7 & 44.5 & 36.0 \\
				& GaitNet~\cite{zhang2019gait}            & RGB & 92.3 & 88.9 & 62.3 \\
				& End2EndGait~\cite{li2020end} 		      & RGB & 97.9 & 93.1 & 77.6 \\
				& GaitMotion~\cite{bashir2009gait}        & OF  & 97.5 & 83.6 & 48.8 \\
				& SM-Prod~\cite{castro2020multimodal}     & Gray+OF	& \bftab{99.8} & \bftab{96.1} & 67.0 \\
				& GQAN(\bftab{ours}) 			          & Sil & 98.51 & 95.37 & \bftab{84.51} \\
				\hline
				\multirow{4}{*}{OUMVLP}
				& GLN-Backbone~\cite{hou2020gait} 		  & Sil  & 94.2 & - & - \\
				& SRN+CBlock~\cite{hou2021setres}         & Sil  & \bftab{96.4} & - & - \\
				& End2EndGait~\cite{li2020end} 		  	  & RGB  & 95.8 & - & - \\
				& GQAN(\bftab{ours}) 				      & Sil  & 96.15 & - & - \\
				\hline
			\end{tabular}
		}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Discussion on Training Tricks}
As described in Section~\ref{sec_settings_backbone}, we adopt some useful tricks to train GQAN-Backbone and achieve the competitive performance.
%
In this section, we try to add these tricks to SRN~\cite{hou2021setres} which holds the best performance before this work.
%
We conduct the experiments on CASIA-B and the performance under different walking conditions is moderately improved (NM-98.01\%, BG-95.09\%, CL-83.34\%) which yet is still inferior to GQAN (NM-98.51\%, BG-95.37\%, CL-84.51\%).
%
More importantly, GQAN can enhance the interpretability of silhouette-based gait recognition by trying to find out the relative importance of each silhouette and each part.
%
% In Section~\ref{sec_settings_backbone}, we have explained the reasons why SRN~\cite{hou2021setres} as well as GaitSet~\cite{chao2019gaitset}, GaitPart~\cite{fan2020gaitpart}, GLN~\cite{hou2020gait} cannot be directly adopted as the backbone for GQAN.
%
% We believe that GQAN can achieve higher accuracy under different walking conditions with the aid of a stronger backbone, which is left for the future work.

\subsubsection{Discussion on Generalization Ability}
In this section, we conduct the experiments to verify the generalization ability of the learned gait quality.
%
Specifically, we perform the frame quality visualization on HID Competition Dataset 2021 using the same settings as those in Section~\ref{sec_frame_vis} except that the model is only trained on CASIA-B.
%
We provide some visualization results randomly selected from HID Competition Dataset 2021 in Fig.~\ref{fig_fquality_hid}, which indicates that the learned gait quality can be generalized to an unseen dataset.

\subsubsection{Discussion on Gait Interpretability}
The interpretability of gait recognition is greatly important for the real-world applications.
%
In this work, we move towards the interpretability of silhouette-based gait recognition by explicitly assessing the quality of each silhouette and each part.
%
However, the problem needs further exploration.
%
For example, the part representations for gait recognition in most works~\cite{chao2019gaitset,fan2020gaitpart,hou2020gait} are obtained by horizontally and equally slicing the features which are inconsistent with the semantic parts of human body, \eg, hands or feet.
%
And the interpretability of model-based gait recognition also needs to be explored where the weights for different key points are not explicitly modeled in the previous works~\cite{liao2020model,li2020end}.
%
Moreover, fusing the multimodal features proves to useful for many other visual tasks~\cite{zhang2020advances,zhang2021improved}.
%
It is also promising to fuse the silhouettes and key points to obtain more rich features for gait recognition, and how to enhance the interpretability in this case remains a challenge.

\ifx\allfiles\undefined
\input{tnnls_suffix}
\fi
